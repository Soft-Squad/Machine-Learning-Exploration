{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5b6075",
   "metadata": {},
   "source": [
    "# Winters, Alexander (V00970263)\n",
    "\n",
    "# Problem 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cc329",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "https://gtraskas.github.io/post/spamit/ ---> To load the data\n",
    "\n",
    "https://www.kdnuggets.com/2020/07/spam-filter-python-naive-bayes-scratch.html\n",
    "\n",
    "https://www.youtube.com/watch?v=O2L2Uv9pdDA\n",
    "\n",
    "https://github.com/Soft-Squad/concordance/blob/main/concord4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50e7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96277ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89c619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to download file, extract and format\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "# SSL Certificate verificaiton failed workaround\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ab0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file\n",
    "url = 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron5.tar.gz'\n",
    "\n",
    "# Get user's download folder path\n",
    "user_downloads = os.path.join(os.path.expanduser('~') + '\\Downloads')\n",
    "enron_dir = os.path.join(user_downloads, 'Enron5-Emails')\n",
    "\n",
    "if not os.path.exists(enron_dir):\n",
    "    os.makedirs(enron_dir)\n",
    "path = os.path.join(enron_dir, 'enron5.tar.gz')\n",
    "if not os.path.exists(path):\n",
    "    urllib.request.urlretrieve(url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40286b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Emails\n",
    "def extract_emails(fname):\n",
    "    rows = []\n",
    "    tfile = tarfile.open(fname, 'r:gz')\n",
    "    for member in tfile.getmembers():\n",
    "        if 'ham' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'LABEL': 'ham', 'MSG': row})\n",
    "        if 'spam' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'LABEL': 'spam', 'MSG': row})\n",
    "    tfile.close()\n",
    "    return pd.DataFrame(rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69cdadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the pickle file is deleted to run again\n",
    "if not os.path.exists(user_downloads + '/emails.pickle'):\n",
    "    df = pd.DataFrame({'LABEL': [], 'MSG': []})\n",
    "    unzipped_file = extract_emails(os.path.join(enron_dir, 'enron5.tar.gz'))\n",
    "    df = pd.concat([df, unzipped_file])\n",
    "    df.to_pickle(user_downloads + '/emails.pickle')\n",
    "        \n",
    "\n",
    "with open(user_downloads + '/emails.pickle', 'rb') as f:\n",
    "    emails_df = pickle.load(f) \n",
    "\n",
    "# Translate bytes objects into strings.\n",
    "df['MSG'] = df['MSG'].apply(lambda x: x.decode('latin-1'))\n",
    "\n",
    "# Reset pandas df index.\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265b4b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>MSG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: leadership development pilot\\r\\nsally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : additional responsibility\\r\\ncon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: global risk management operations\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: global risk management operations\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: global risk management operations\\r\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LABEL                                                MSG\n",
       "0   ham  Subject: leadership development pilot\\r\\nsally...\n",
       "1   ham  Subject: re : additional responsibility\\r\\ncon...\n",
       "2   ham  Subject: global risk management operations\\r\\n...\n",
       "3   ham  Subject: global risk management operations\\r\\n...\n",
       "4   ham  Subject: global risk management operations\\r\\n..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ff0462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5175, 2)\n",
      "spam    0.710145\n",
      "ham     0.289855\n",
      "Name: LABEL, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df['LABEL'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb3a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data for 70/30 split into training and validation sets\n",
    "shuffled_dataset = df.sample(frac=1).reset_index(drop=True)\n",
    "split = int(df.shape[0] * 0.7)\n",
    "\n",
    "train_set = shuffled_dataset.iloc[:split].reset_index(drop=True)\n",
    "# Remove anything that is not a word\n",
    "train_set['MSG'] = train_set['MSG'].str.lower().replace('[^\\sa-zA-Z]+', ' ', regex=True)\n",
    "train_set['MSG'] = train_set['MSG'].str.split()\n",
    "\n",
    "test_set = shuffled_dataset.iloc[split:].reset_index(drop=True)\n",
    "test_set['MSG'] = test_set['MSG'].str.lower().replace('[^\\sa-zA-Z]+', ' ', regex=True)\n",
    "test_set['MSG'] = test_set['MSG'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4f1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_set['MSG']\n",
    "train_y = train_set['LABEL']\n",
    "\n",
    "test_X = test_set['MSG']\n",
    "test_y = test_set['LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f09a3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_X, train_y):\n",
    "    # Remove duplicate words\n",
    "    unique_words = []\n",
    "    for key, email in train_X.iteritems():\n",
    "        for word in email:\n",
    "            unique_words.append(word)\n",
    "    unique_words = sorted(list(set(unique_words)))\n",
    "    \n",
    "    email_word_counts = {unique_word: [0] * len(train_X) for unique_word in unique_words}\n",
    "    # Find count of each word in email\n",
    "    for index, email in enumerate(train_X):\n",
    "        for word in email:\n",
    "            email_word_counts[word][index] += 1\n",
    "    \n",
    "    # Store word counts in a df\n",
    "    word_counts = pd.DataFrame(email_word_counts)\n",
    "    \n",
    "    clean_df = pd.concat([train_X, word_counts], axis=1)\n",
    "    \n",
    "    # Get all spam and ham emails\n",
    "    spam_emails = clean_df.iloc[np.where(train_y == 'spam')[0]]\n",
    "    ham_emails = clean_df.iloc[np.where(train_y == 'ham')[0]]\n",
    "    \n",
    "    # Get the prob of an email being spam and ham\n",
    "    p_spam = len(spam_emails) / len(clean_df)\n",
    "    p_ham = len(ham_emails) / len(clean_df)\n",
    "    \n",
    "    p_wi_spam = {unique_word: 0 for unique_word in unique_words}\n",
    "    p_wi_ham = {unique_word: 0 for unique_word in unique_words}\n",
    "    \n",
    "    # Find conditional prob. of each word\n",
    "    for word in unique_words:\n",
    "        # P(wi|spam) and P(wi|ham)\n",
    "        p_wi_spam[word] = spam_emails[word].sum() / len(spam_emails)\n",
    "        p_wi_ham[word] = ham_emails[word].sum() / len(ham_emails)\n",
    "    return p_wi_spam, p_wi_ham, p_spam, p_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4be331d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_X, test_y, p_wi_spam, p_wi_ham, p_spam, p_ham):\n",
    "    correct = 0\n",
    "    \n",
    "    for index, email in test_X.iteritems():\n",
    "        test_p_wi_spam = []\n",
    "        test_p_wi_ham = []\n",
    "        \n",
    "        true_value = test_y.loc[index]\n",
    "        y_pred = ''\n",
    "        \n",
    "        for word in email:\n",
    "            if word in p_wi_spam:\n",
    "                test_p_wi_spam.append(p_wi_spam[word])\n",
    "            if word in p_wi_ham:\n",
    "                test_p_wi_ham.append(p_wi_ham[word])\n",
    "        \n",
    "        # Get prob. of a given email\n",
    "        p_w_spam = p_spam * np.prod(test_p_wi_spam)\n",
    "        p_w_ham = p_ham * np.prod(test_p_wi_ham)\n",
    "        \n",
    "        # Check which prob. is higher and provide a label\n",
    "        if p_w_spam > p_w_ham:\n",
    "            y_pred = 'spam'\n",
    "        elif p_w_ham > p_w_spam: \n",
    "            y_pred = 'ham'\n",
    "            \n",
    "        if y_pred == true_value:\n",
    "            correct += 1\n",
    "        \n",
    "    acc = (correct / len(test_X)) * 100\n",
    "    err = 100. - acc\n",
    "    return acc, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac3914f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "\n",
      "Training Accuracy: 88.85% Training Error: 11.15%\n",
      "Test Accuracy: 68.32% Test Error: 31.68%\n",
      "Total Time: 75.17s\n"
     ]
    }
   ],
   "source": [
    "bayes_time = time.time()\n",
    "p_wi_spam, p_wi_ham, p_spam, p_ham = fit(train_X, train_y)\n",
    "\n",
    "train_acc, train_err = predict(train_X, train_y, p_wi_spam, p_wi_ham, p_spam, p_ham)\n",
    "test_acc, test_err = predict(test_X, test_y, p_wi_spam, p_wi_ham, p_spam, p_ham)\n",
    "\n",
    "print(\"Naive Bayes\\n\")\n",
    "print(\"Training Accuracy: {:.2f}% Training Error: {:.2f}%\".format(train_acc, train_err))\n",
    "print(\"Test Accuracy: {:.2f}% Test Error: {:.2f}%\".format(test_acc, test_err))\n",
    "print(\"Total Time: {:.2f}s\".format(time.time() - bayes_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd12c43",
   "metadata": {},
   "source": [
    "It is moderately accurate on test data with ~68% accuracy. I think if we added an alpha value to each word_count, we might see better results due to some sneaky emails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
